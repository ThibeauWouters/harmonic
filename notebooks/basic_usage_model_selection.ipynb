{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic usage with model selection\n",
    "\n",
    "In this tutorial we demonstrate a further usage of harmonic, using emcee as the sampler. Additionally we perform cross-validation to select the best model in addition to optimizing the hyperparameters of said model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "\n",
    "To begin with we need to import packages needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import emcee\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "import harmonic as hm\n",
    "sys.path.append(\"../examples\")\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Bayesian posterior function\n",
    "\n",
    "Now we will need to define the log-posterior function of interest. \n",
    "\n",
    "As a working example for this tutorial we consider a likelihood given by the Rastrigin function\n",
    "\n",
    "$$\n",
    "f(x) = 10 d + \\sum_{i=1}^{d} \\bigg [ x_i^2 - 10 \\cos ( 2 \\pi x_i ) \\bigg ]\n",
    "$$\n",
    "\n",
    "where $d$ is the dimension of the function and the input domain is usually taken to be $x_i \\in [-6.0, 6.0], \\: \\; \\forall i = 1, \\dots, d$.  The Rastrigin function is a common benchmark example since it is known to be a difficult project due to its high multimodality.  The likelihood is then implemented as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_likelihood(x):\n",
    "    \"\"\"Compute log_e of likelihood defined by Rastrigin function.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        x: Position at which to evaluate likelihood.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        double: Value of Rastrigin at specified point.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    ndim = x.size\n",
    "\n",
    "    f = 10.0 * ndim\n",
    "\n",
    "    for i_dim in range(ndim):\n",
    "        f += x[i_dim]**2 - 10.0 * np.cos( 2.0 * np.pi * x[i_dim] )\n",
    "\n",
    "    return -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We adopt a uniform prior over the paramter support $x_i \\in [-6.0, 6.0]$, which is implemented as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_prior_uniform(x, xmin=-6.0, xmax=6.0, ymin=-6.0, ymax=6.0):\n",
    "    \"\"\"Compute log_e of uniform prior.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        x: Position at which to evaluate prior.\n",
    "\n",
    "        xmin: Uniform prior minimum x edge (first dimension).\n",
    "\n",
    "        xmax: Uniform prior maximum x edge (first dimension).\n",
    "\n",
    "        ymin: Uniform prior minimum y edge (second dimension).\n",
    "\n",
    "        ymax: Uniform prior maximum y edge (second dimension).\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        double: Value of prior at specified point.\n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "    if x[0] >= xmin and x[0] <= xmax and x[1] >= ymin and x[1] <= ymax:        \n",
    "        return 1.0 / ( (xmax - xmin) * (ymax - ymin) )\n",
    "    else:\n",
    "        return 0.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood and prior are combined to form the log posterior function as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_posterior(x, ln_prior):\n",
    "    \"\"\"Compute log_e of posterior.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        x: Position at which to evaluate posterior.\n",
    "\n",
    "        ln_prior: Prior function.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        double: Posterior at specified point.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    ln_L = ln_likelihood(x)\n",
    "\n",
    "    if not np.isfinite(ln_L):\n",
    "        return -np.inf\n",
    "    else:\n",
    "        return ln_prior(x) + ln_L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute samples using emcee\n",
    "\n",
    "We then sample from the posterior using an MCMC algorithm. While any MCMC approach can be used we sample using the [emcee](https://emcee.readthedocs.io/en/stable/) package.\n",
    "\n",
    "First we will need to define and initialize some variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for emcee sampling\n",
    "ndim = 2                    # number of dimensions\n",
    "nchains = 200               # total number of chains to compute\n",
    "samples_per_chain = 5000    # number of samples per chain\n",
    "nburn = 2000                # number of samples to discard as burn in\n",
    "\n",
    "# Initialize random seed\n",
    "np.random.seed(20)\n",
    "\n",
    "# Define ln_prior function\n",
    "xmin = -6.0\n",
    "xmax = 6.0\n",
    "ymin = -6.0\n",
    "ymax = 6.0  \n",
    "ln_prior = partial(ln_prior_uniform, xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to run the sampler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial random position and state\n",
    "pos = np.random.rand(ndim * nchains).reshape((nchains, ndim)) * 0.5  \n",
    "rstate = np.random.get_state()\n",
    "\n",
    "# Instantiate and execute sampler \n",
    "sampler = emcee.EnsembleSampler(nchains, ndim, ln_posterior, args=[ln_prior])\n",
    "(pos, prob, state) = sampler.run_mcmc(pos, samples_per_chain, rstate0=rstate) \n",
    "\n",
    "# Collect samples into contiguous numpy arrays (discarding burn in)\n",
    "samples = np.ascontiguousarray(sampler.chain[:,nburn:,:])\n",
    "lnprob = np.ascontiguousarray(sampler.lnprobability[:,nburn:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute evidence using harmonic\n",
    "\n",
    "The harmonic package requires only posterior samples.  There are no constraints on the type of sampling algorithm used.\n",
    "\n",
    "Once we have posterior samples to hand, they can be post-processed using harmonic to compute the Bayesian evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collating samples using harmonic.chains class\n",
    "\n",
    "We first configure the chains into a harmonic-friendly shape, which we do as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate harmonic's chains class \n",
    "chains = hm.Chains(ndim)\n",
    "chains.add_chains_3d(samples, lnprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will subsequently learn the target distribution $\\varphi$ we split the samples into training and inference sets (we often use the common machine learning terminology \"test\" for the inference data-set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the chains into the ones which will be used to train the machine \n",
    "# learning model and for inference\n",
    "chains_train, chains_infer = hm.utils.split_data(chains, training_proportion=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select and train the optimal machine learning model\n",
    "\n",
    "Now consider `chains_train` and use the chains to train the model. Here we will leverage cross-validation to select the best model (from those provided).\n",
    "\n",
    "We will perform cross validation to select the most appropriate model and then optimize the hyperparameters of said model.  First we define the sets of hyper-parameters for each model,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Kernel Density Estimate (KDE) hyperparameters\n",
    "nfold = 2\n",
    "nhyper = 2\n",
    "step = -2\n",
    "domains_KDE = [] # no defined domain for KDE estimate\n",
    "hyper_parameters_KDE = [[10**(R)] for R in range(-nhyper+step,step)] \n",
    "\n",
    "# Define Modified Gaussian Mixtures Model (MGMM) hyperparameters\n",
    "hyper_parameters_MGMM = [[1, 1E-8, 0.1, 6, 10], \n",
    "                         [2, 1E-8, 0.5, 6, 20],\n",
    "                         [3, 1E-8, 0.25, 6, 30]]\n",
    "domains_MGMM = [np.array([1E-2,1E1])]\n",
    "\n",
    "# Define Hypersphere (sphere) hyperparameters\n",
    "hyper_parameters_sphere = [None]\n",
    "domains_sphere = [np.array([1E-2,1E1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation is then performing using harmonic untils to select the best hyper-parameter configuration for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_variances_KDE = \\\n",
    "            hm.utils.cross_validation(\n",
    "                    chains_train, \\\n",
    "                    domains_KDE, \\\n",
    "                    hyper_parameters_KDE, \\\n",
    "                    nfold=nfold, \\\n",
    "                    modelClass=hm.model.KernelDensityEstimate, \\\n",
    "                    verbose=False, \\\n",
    "                    seed=0)\n",
    "best_hyper_param_ind_KDE = np.argmin(validation_variances_KDE)\n",
    "best_hyper_param_KDE = hyper_parameters_KDE[best_hyper_param_ind_KDE]\n",
    "best_var_KDE = validation_variances_KDE[best_hyper_param_ind_KDE]\n",
    "\n",
    "validation_variances_sphere = \\\n",
    "            hm.utils.cross_validation(\n",
    "                    chains_train, \\\n",
    "                    domains_sphere, \\\n",
    "                    hyper_parameters_sphere, \\\n",
    "                    nfold=nfold, \\\n",
    "                    modelClass=hm.model.HyperSphere, \\\n",
    "                    verbose=False, \\\n",
    "                    seed=0)\n",
    "\n",
    "best_hyper_param_ind_sphere = np.argmin(validation_variances_sphere)\n",
    "best_hyper_param_sphere = hyper_parameters_sphere[best_hyper_param_ind_sphere]\n",
    "best_var_sphere = validation_variances_sphere[best_hyper_param_ind_sphere]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the minimal validation variances for each model (over the given domains) we can compare them to one another to select the model with best performing model,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_var_sphere < best_var_KDE:                        \n",
    "        model = hm.model.HyperSphere(ndim, domains_sphere, hyper_parameters=best_hyper_param_sphere)\n",
    "        print('Using Hypersphere model!')\n",
    "  \n",
    "else:                       \n",
    "        model = hm.model.KernelDensityEstimate(ndim, domains_KDE, hyper_parameters=best_hyper_param_KDE)\n",
    "        print('Using Kernel Density Estimate model!')\n",
    "\n",
    "model.verbose=False\n",
    "\n",
    "print(best_var_sphere, best_var_KDE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we simply train the automatically selected model using the optimized hyper-parameters and the training chains generated previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_success = model.fit(chains_train.samples, chains_train.ln_posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Bayesian evidence\n",
    "\n",
    "Finally we simply compute the learnt harmonic mean estimator as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate harmonic's evidence class\n",
    "ev = hm.Evidence(chains_infer.nchains, model)\n",
    "\n",
    "# Pass the evidence class the inference chains and compute the evidence!\n",
    "ev.add_chains(chains_infer)\n",
    "evidence, evidence_std = ev.compute_evidence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Let's check the evidence value computed and also plot the posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical integration\n",
    "\n",
    "For this 2D model, we can compute the evidence by brute force numerical integration to compare to the value computed by harmonic. (Of course, numerical integration is not typically possible for higher dimensional models.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_posterior_func = partial(ln_posterior, ln_prior=ln_prior)\n",
    "ln_posterior_grid, x_grid, y_grid = utils.eval_func_on_grid(\n",
    "                                        ln_posterior_func, \n",
    "                                        xmin=xmin, xmax=xmax, \n",
    "                                        ymin=ymin, ymax=ymax, \n",
    "                                        nx=1000, ny=1000)\n",
    "dx = x_grid[0,1] - x_grid[0,0]\n",
    "dy = y_grid[1,0] - y_grid[0,0]\n",
    "evidence_numerical = np.sum(np.exp(ln_posterior_grid))*dx*dy\n",
    "ln_evidence_numerical = np.log(evidence_numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the value computed by harmonic and by numerical integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('evidence (harmonic) = {} +/- {}'.format(evidence, evidence_std))\n",
    "print('evidence (numerical integration) = {}'.format(evidence_numerical))\n",
    "print('nsigma = {}'.format(np.abs(evidence - evidence_numerical) / evidence_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the evidence computed by harmonic is close to that computed by numerical integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior plot\n",
    "\n",
    "Out of interest let's also plot the posterior using these samples to see what we're working with!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the surface defined by the posterior evaluated over the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = utils.plot_surface(ln_posterior_grid, x_grid, y_grid, \n",
    "                        samples[0,:,:].reshape((-1, ndim)), \n",
    "                        lnprob[0,:].reshape((-1, 1)))              \n",
    "ax.set_zlabel(r'$\\log \\mathcal{L}$')\n",
    "ax.view_init(30,100)\n",
    "plt.show(block=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can plot using a triangle plot of joint and marginalised distibutions using [getdist](https://getdist.readthedocs.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_getdist(samples.reshape((-1, ndim)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
