{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages\n",
    "\n",
    "To begin with we need to import packages to work with, these being:\n",
    "\n",
    "1. harmonic - for bayesian evidence computation.\n",
    "2. numpy - for basic math functions.\n",
    "3. emcee - for MCMC sampling (can be replaced by any preferred sampling package). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-07-12 17:50:37,122] [Harmonic] [CRITICAL]: \u001b[1;31;40mUsing config from /Users/matt/Downloads/Software/harmonic/logs/logging.yaml\u001b[0;0m\n",
      "[2020-07-12 17:50:37,301] [Harmonic] [CRITICAL]: \u001b[1;31;40mUsing config from /Users/matt/Downloads/Software/harmonic/logs/logging.yaml\u001b[0;0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import emcee\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from functools import partial\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/matt/Downloads/Software/harmonic')\n",
    "import harmonic as hm\n",
    "sys.path.append(\"../examples\")\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Bayesian Posterior Function\n",
    "\n",
    "Now we will need to define the log-posterior function we are interested in. Here we consider perhaps the most basic case of a n-dimensional uniform prior with a simple Gaussian likelihood defined by \n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\boldsymbol{x}) = \\frac{1}{2\\pi \\: \\text{det} \\boldsymbol{\\Sigma}^{-\\frac{1}{2}}}e^{-\\frac{1}{2} \\boldsymbol{x}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{x}},\n",
    "$$\n",
    "for $\\boldsymbol{x} \\in \\mathbb{R}^n$. This is simply coded as\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_posterior(x, inv_cov):\n",
    "    \"\"\"\n",
    "    Compute log_e of posterior.\n",
    "    Args: \n",
    "        - x: \n",
    "            Position at which to evaluate posterior.\n",
    "        - inv_cov: \n",
    "            Inverse covariance matrix.      \n",
    "    Returns:\n",
    "        - double: \n",
    "            Value of Gaussian at specified point.\n",
    "    \"\"\"\n",
    "    \n",
    "    return -np.dot(x,np.dot(inv_cov,x))/2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for non-diagonal covariance matrix $\\boldsymbol{\\Sigma}$ with randomized entries computed by the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_cov(ndim):\n",
    "    \"\"\"\n",
    "    Initialise random non-diagonal covariance matrix.\n",
    "    Args: \n",
    "        - ndim: \n",
    "            Dimension of Gaussian.           \n",
    "    Returns:\n",
    "        - cov: \n",
    "            Covariance matrix of shape (ndim,ndim).\n",
    "    \"\"\"\n",
    "    \n",
    "    cov = np.zeros((ndim,ndim))\n",
    "    diag_cov = np.ones(ndim) + np.random.randn(ndim)*0.1\n",
    "    np.fill_diagonal(cov, diag_cov)\n",
    "    \n",
    "    for i in range(ndim-1):\n",
    "        cov[i,i+1] = (-1)**i * 0.5*np.sqrt(cov[i,i]*cov[i+1,i+1])\n",
    "        cov[i+1,i] = cov[i,i+1]\n",
    "    return cov \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytic Evidence\n",
    "\n",
    "Finally we require the analytic evidence to compare our estimate against. Thankfully this is straightforward in this setting and is given by \n",
    "\n",
    "$$\n",
    "\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}\\mathcal{L}(\\boldsymbol{x})d\\boldsymbol{x} = \\frac{(2\\pi)^{\\frac{n}{2}}}{\\sqrt{\\:\\text{det}\\boldsymbol{\\Sigma}}}\n",
    "$$\n",
    "\n",
    "which can be coded in python as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_analytic_evidence(ndim, cov):\n",
    "    \"\"\"\n",
    "    Compute analytic evidence for nD Gaussian.\n",
    "    Args:\n",
    "        - ndim: \n",
    "            Dimension of Gaussian.\n",
    "        - cov: \n",
    "            Covariance matrix.\n",
    "    Returns:\n",
    "        - double:\n",
    "            Analytic evidence.\n",
    "    \"\"\"\n",
    "    \n",
    "    ln_norm_lik = 0.5*ndim*np.log(2*np.pi) + 0.5*np.log(np.linalg.det(cov))\n",
    "    return ln_norm_lik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute samples using Emcee\n",
    "\n",
    "In typical fashion we now need to collect some samples of the posterior using whichever MCMC package you wish to use. Here we'll collect samples using the [emcee](https://emcee.readthedocs.io/en/stable/) package.\n",
    "\n",
    "First we will need to define and initialize some variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for emcee sampling\n",
    "ndim = 2                                     # number of dimensions\n",
    "nchains = 100                                # total number of chains to compute\n",
    "samples_per_chain = 5000                     # number of samples per chain\n",
    "nburn = 500                                  # number of samples to discard as burn in\n",
    "\n",
    "# Initialize random seed\n",
    "np.random.seed(10)\n",
    "\n",
    "# Create random non-diagonal inverse covariance\n",
    "cov = init_cov(ndim)\n",
    "inv_cov = np.linalg.inv( cov )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to run the sampler to collect samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial random position and state\n",
    "pos = np.random.rand(ndim * nchains).reshape((nchains, ndim))\n",
    "rstate = np.random.get_state()\n",
    "\n",
    "# Instantiate and execute Sampler \n",
    "sampler = emcee.EnsembleSampler(nchains, ndim, ln_posterior, args=[inv_cov])\n",
    "(pos, prob, state) = sampler.run_mcmc(pos, samples_per_chain, rstate0=rstate) \n",
    "\n",
    "# Collect samples into contiguous numpy arrays (discarding burn in)\n",
    "samples = np.ascontiguousarray(sampler.chain[:,nburn:,:])\n",
    "lnprob = np.ascontiguousarray(sampler.lnprobability[:,nburn:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harmonic Code\n",
    "\n",
    "Here is where one would normally start (*i.e.* already with chains ready to compute the evidence).\n",
    "\n",
    "## Collating samples using harmonic.chains class\n",
    "\n",
    "Now we simply need to configure the chains into a harmonic friendly shape which we do by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate harmonic's chains class \n",
    "chains = hm.Chains(ndim)\n",
    "chains.add_chains_3d(samples, lnprob)\n",
    "\n",
    "# Split the chains into the ones which will be used to train/test the machine learning model\n",
    "chains_train, chains_test = hm.utils.split_data(chains, training_proportion=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the machine learning model\n",
    "\n",
    "Now take the chains_train from the above code and use them to train the model (here we have selected the HyperSphere model for simplicity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model domain over which to optimize the hyper-parameters\n",
    "domains = [np.sqrt(ndim-1)*np.array([1E-1,1E0])]\n",
    "\n",
    "# Now simply instantiate the model class you wish and train it\n",
    "model = hm.model.HyperSphere(ndim, domains)\n",
    "fit_success, objective = model.fit(chains_train.samples, chains_train.ln_posterior) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Bayesian evidence\n",
    "\n",
    "Finally we simply compute the Learnt Harmonic Mean estimator by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate harmonic's evidence class\n",
    "ev = hm.Evidence(chains_test.nchains, model)\n",
    "\n",
    "# Pass the evidence class the test chains and compute the evidence!\n",
    "ev.add_chains(chains_test)\n",
    "ln_evidence, ln_evidence_std = ev.compute_ln_evidence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis\n",
    "\n",
    "Lets take a look at what the error on our estimate is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytic value: 1.791083315460074 || Learnt Harmonic Mean value: 1.7932610258353037\n",
      "Number of sigma away from true value: 0.5226588159689041\n"
     ]
    }
   ],
   "source": [
    "# Compute the analytic evidence\n",
    "ln_evidence_analytic = ln_analytic_evidence(ndim, cov)\n",
    "\n",
    "print('Analytic value: {} || Learnt Harmonic Mean value: {}'.format(ln_evidence_analytic, ln_evidence))\n",
    "\n",
    "# Compare to the learnt harmonic mean estimate \n",
    "abs_error = np.abs(ln_evidence_analytic - ln_evidence)\n",
    "percent_error = 100.0 * abs_error / ln_evidence_analytic\n",
    "\n",
    "# Compare the variance provided by the learnt harmonic mean estimate\n",
    "sigma_error = np.log(np.exp(ln_evidence) + np.exp(ln_evidence_std)) - ln_evidence\n",
    "\n",
    "sigma_count = abs_error / sigma_error\n",
    "\n",
    "print('Number of sigma away from true value: {}'.format(sigma_count))\n",
    "\n",
    "\n",
    "# N.B: sigma_count represents the number of standard deviations between the estimated \n",
    "#     and analytic results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
