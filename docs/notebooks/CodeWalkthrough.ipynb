{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages\n",
    "\n",
    "To begin with we need to import packages to work with, these being:\n",
    "\n",
    "1. harmonic - for bayesian evidence computation.\n",
    "2. numpy - for basic math functions.\n",
    "3. emcee - for MCMC sampling (can be replaced by any preferred sampling package). \n",
    "4. functools.partial - for numerical integration. \n",
    "5. matplotlib - for data visualisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import emcee\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from functools import partial\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/matt/Downloads/Software/harmonic')\n",
    "import harmonic as hm\n",
    "sys.path.append(\"../examples\")\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Bayesian Posterior Function\n",
    "\n",
    "Now we will need to define the log-posterior function we are interested in. Here we consider the interesting case of the pathological Rosenbrock function, \n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^{d-1} \\bigg [ 100(x_{i+1} - x_{i}^2)^2 + (x_i - 1)^2 \\bigg ]\n",
    "$$\n",
    "where $d$ is the dimension of the function and the input domain is usually taken to be $x_i \\in [-5.0, 10.0] \\: \\; \\forall i = 1, \\dots, d$. The Rosenbrock function is difficult in the sense that convergence to the unimodal minimum is difficult. \n",
    "\n",
    "To compliment this choice of likelihood we adopt a uniform prior over the covariate support $x_i \\in [-5.0, 10.0]$. This may simply be coded as the prior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_prior_uniform(x, xmin=-10.0, xmax=10.0, ymin=-5.0, ymax=15.0):\n",
    "    \"\"\"\n",
    "    Compute log_e of uniform prior.\n",
    "    Args: \n",
    "        - x: \n",
    "            Position at which to evaluate prior.\n",
    "        - xmin: \n",
    "            Uniform prior minimum x edge (first dimension).\n",
    "        - xmax: \n",
    "            Uniform prior maximum x edge (first dimension).\n",
    "        - ymin: \n",
    "            Uniform prior minimum y edge (second dimension).\n",
    "        - ymax: \n",
    "            Uniform prior maximum y edge (second dimension).             \n",
    "    Returns:\n",
    "        - double: \n",
    "            Value of prior at specified point.\n",
    "    \"\"\"\n",
    "        \n",
    "    if x[0] >= xmin and x[0] <= xmax and x[1] >= ymin and x[1] <= ymax:        \n",
    "        return 1.0 / ( (xmax - xmin) * (ymax - ymin) )\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the Rosenbrock likelihood "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ln_likelihood(x, a=1.0, b=100.0):\n",
    "    \"\"\"\n",
    "    Compute log_e of likelihood defined by Rosenbrock function.\n",
    "    Args: \n",
    "        - x: \n",
    "            Position at which to evaluate likelihood.\n",
    "        - a: \n",
    "            First parameter of Rosenbrock function.   \n",
    "        - b: \n",
    "            First parameter of Rosenbrock function. \n",
    "    Returns:\n",
    "        - double: \n",
    "            Value of Rosenbrock at specified point.\n",
    "    \"\"\"\n",
    "    \n",
    "    ndim = x.size\n",
    "\n",
    "    f = 0.0\n",
    "\n",
    "    for i_dim in range(ndim-1):\n",
    "        f += b*(x[i_dim+1]-x[i_dim]**2)**2 + (a-x[i_dim])**2\n",
    "\n",
    "    return -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is combined into the log posterior function given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_posterior(x, ln_prior, a=1.0, b=100.0):\n",
    "    \"\"\"\n",
    "    Compute log_e of posterior.\n",
    "    Args: \n",
    "        - x: \n",
    "            Position at which to evaluate posterior.\n",
    "        - a: \n",
    "            First parameter of Rosenbrock function.   \n",
    "        - b: \n",
    "            First parameter of Rosenbrock function.\n",
    "        - ln_prior: \n",
    "            Prior function. \n",
    "    Returns:\n",
    "        - double: \n",
    "            Posterior at specified point.\n",
    "    \"\"\"\n",
    "    \n",
    "    ln_L = ln_likelihood(x, a=a, b=b)\n",
    "\n",
    "    if not np.isfinite(ln_L):\n",
    "        return -np.inf\n",
    "    else:\n",
    "        return ln_prior(x) + ln_L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for hyper-parameters and b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute samples using Emcee\n",
    "\n",
    "In typical fashion we now need to collect some samples of the posterior using whichever MCMC package you wish to use. Here we'll collect samples using the [emcee](https://emcee.readthedocs.io/en/stable/) package.\n",
    "\n",
    "First we will need to define and initialize some variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for emcee sampling\n",
    "ndim = 2                    # number of dimensions\n",
    "nchains = 200               # total number of chains to compute\n",
    "samples_per_chain = 5000    # number of samples per chain\n",
    "nburn = 2000                # number of samples to discard as burn in\n",
    "\n",
    "# Initialize random seed\n",
    "np.random.seed(10)\n",
    "\n",
    "# Rosenbrock hyper-parameters\n",
    "a = 1.0\n",
    "b = 100.0\n",
    "\n",
    "# Define ln_prior function\n",
    "xmin = -10.0\n",
    "xmax = 10.0\n",
    "ymin = -5.0\n",
    "ymax = 15.0  \n",
    "ln_prior = partial(ln_prior_uniform, xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to run the sampler to collect samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial random position and state\n",
    "pos = np.random.rand(ndim * nchains).reshape((nchains, ndim)) * 0.1   \n",
    "rstate = np.random.get_state()\n",
    "\n",
    "# Instantiate and execute Sampler \n",
    "sampler = emcee.EnsembleSampler(nchains, ndim, ln_posterior, args=[ln_prior, a, b])\n",
    "(pos, prob, state) = sampler.run_mcmc(pos, samples_per_chain, rstate0=rstate) \n",
    "\n",
    "# Collect samples into contiguous numpy arrays (discarding burn in)\n",
    "samples = np.ascontiguousarray(sampler.chain[:,nburn:,:])\n",
    "lnprob = np.ascontiguousarray(sampler.lnprobability[:,nburn:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harmonic Code\n",
    "\n",
    "Here is where one would normally start (*i.e.* already with chains ready to compute the evidence).\n",
    "\n",
    "## Collating samples using harmonic.chains class\n",
    "\n",
    "Now we simply need to configure the chains into a harmonic friendly shape which we do by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate harmonic's chains class \n",
    "chains = hm.Chains(ndim)\n",
    "chains.add_chains_3d(samples, lnprob)\n",
    "\n",
    "# Split the chains into the ones which will be used to train/test the machine learning model\n",
    "chains_train, chains_test = hm.utils.split_data(chains, training_proportion=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the machine learning model\n",
    "\n",
    "Now take the chains_train from the above code and use them to train the model, here we select the Kernel Density estimate. \n",
    "\n",
    "First we must define the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model hyper-parameters and domain\n",
    "nfold = 2\n",
    "nhyper = 2\n",
    "step = -2\n",
    "domain = []\n",
    "hyper_parameters = [[10**(R)] for R in range(-nhyper+step,step)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wish to optimize the hyper-parameters over the domain. We do this *via* cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_variances = \\\n",
    "            hm.utils.cross_validation(\n",
    "                    chains_train, \\\n",
    "                    domain, \\\n",
    "                    hyper_parameters, \\\n",
    "                    nfold=nfold, \\\n",
    "                    modelClass=hm.model.KernelDensityEstimate, \\\n",
    "                    verbose=False, \\\n",
    "                    seed=0)\n",
    "\n",
    "best_hyper_param_ind = np.argmin(validation_variances)\n",
    "best_hyper_param = hyper_parameters[best_hyper_param_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we simply instantiate the model and train it using the optimized hyper-parameters and the training chains generated previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hm.model.KernelDensityEstimate(ndim, \n",
    "                                       domain, \n",
    "                                       hyper_parameters=best_hyper_param)\n",
    "fit_success = model.fit(chains_train.samples, chains_train.ln_posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Bayesian evidence\n",
    "\n",
    "Finally we simply compute the Learnt Harmonic Mean estimator by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate harmonic's evidence class\n",
    "ev = hm.Evidence(chains_test.nchains, model)\n",
    "\n",
    "# Pass the evidence class the test chains and compute the evidence!\n",
    "ev.add_chains(chains_test)\n",
    "ln_evidence, ln_evidence_std = ev.compute_ln_evidence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Integration\n",
    "\n",
    "Now for comparison we quickly compute a brute force numerical integration over the posterior for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_posterior_func = partial(ln_posterior, ln_prior=ln_prior, a=a, b=b)\n",
    "ln_posterior_grid, x_grid, y_grid = utils.eval_func_on_grid(\n",
    "                                        ln_posterior_func, \n",
    "                                        xmin=-10.0, xmax=10.0, \n",
    "                                        ymin=-5.0, ymax=15.0, \n",
    "                                        nx=1000, ny=1000)\n",
    "dx = x_grid[0,1] - x_grid[0,0]\n",
    "dy = y_grid[1,0] - y_grid[0,0]\n",
    "evidence_numerical = np.sum(np.exp(ln_posterior_grid))*dx*dy\n",
    "ln_evidence_numerical = np.log(evidence_numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior plot\n",
    "\n",
    "Out of interest why don't we plot the posterior from these samples to see what we're working with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = utils.plot_surface(ln_posterior_grid, x_grid, y_grid, \n",
    "                        samples[0,:,:].reshape((-1, ndim)), \n",
    "                        lnprob[0,:].reshape((-1, 1)))              \n",
    "ax.set_zlabel(r'$\\log \\mathcal{L}$')\n",
    "ax.view_init(30,100)\n",
    "plt.show(block=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with 2D projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot posterior image.\n",
    "ax = utils.plot_image(np.exp(ln_posterior_grid), x_grid, y_grid, \n",
    "                      samples.reshape((-1,ndim)),\n",
    "                      colorbar_label=r'$\\mathcal{L}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis\n",
    "\n",
    "Lets take a look at what the error on our estimate is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Numerical integration value: {} || Learnt Harmonic Mean value: {}'.format(\n",
    "    ln_evidence_numerical, ln_evidence))\n",
    "\n",
    "# Compare to the learnt harmonic mean estimate \n",
    "abs_error = np.abs(ln_evidence_numerical - ln_evidence)\n",
    "percent_error = 100.0 * abs_error / ln_evidence_numerical\n",
    "\n",
    "# Compare the variance provided by the learnt harmonic mean estimate\n",
    "sigma_error = np.log(np.exp(ln_evidence) + np.exp(ln_evidence_std)) - ln_evidence\n",
    "\n",
    "sigma_count = abs_error / sigma_error\n",
    "\n",
    "print('Number of sigma away from true value: {}'.format(sigma_count))\n",
    "\n",
    "# N.B: sigma_count represents the number of standard deviations between the estimated \n",
    "#     and analytic results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
