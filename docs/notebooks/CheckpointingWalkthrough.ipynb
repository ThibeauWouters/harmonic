{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Disclaimer**\n",
    "*This example assumes you have already worked through the CodeWalkthrough notebook and have a decent understanding of how each part fits together.*\n",
    "\n",
    "During high dimensional (computationally heavy) Bayesian analysis one often must run sampling and/or evidence estimation for long periods of time. One issue often encountered is that the computing facilities may go offline during this period, thus discarding any values computed until this disconnection. To avoid this issue Harmonic supports what is called *Checkpointing* which allows the user to periodically *back-up* the progress of the computation. This example should hopefully illustrate how one may do this in practise.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic problem setup steps\n",
    "\n",
    "1. As always import relevant python modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import emcee\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from functools import partial\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/matt/Downloads/Software/harmonic')\n",
    "import harmonic as hm\n",
    "sys.path.append(\"../examples\")\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define a Bayesian Posterior Function (here we'll use a simple gaussian example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_analytic_evidence(ndim, cov):\n",
    "    \"\"\"\n",
    "    Compute analytic ln_e evidence.\n",
    "    Args: \n",
    "        - ndim: \n",
    "            Dimensionality of the multivariate Gaussian posterior\n",
    "        - cov\n",
    "            Covariance matrix dimension nxn.           \n",
    "    Returns:\n",
    "        - double: \n",
    "            Value of posterior at x.\n",
    "    \"\"\"\n",
    "    ln_norm_lik = -0.5*ndim*np.log(2*np.pi)-0.5*np.log(np.linalg.det(cov))   \n",
    "    return -ln_norm_lik\n",
    "\n",
    "def ln_Posterior(x, inv_cov):\n",
    "    \"\"\"\n",
    "    Compute log_e of n dimensional multivariate gaussian \n",
    "    Args: \n",
    "        - x: \n",
    "            Position at which to evaluate prior.         \n",
    "    Returns:\n",
    "        - double: \n",
    "            Value of posterior at x.\n",
    "    \"\"\"\n",
    "    return -np.dot(x,np.dot(inv_cov,x))/2.0   \n",
    "\n",
    "def init_cov(ndim): \n",
    "    \"\"\"\n",
    "    Initialise random diagonal covariance matrix.\n",
    "    Args: \n",
    "        - ndim: \n",
    "            Dimension of Gaussian.        \n",
    "    Returns:\n",
    "        - cov: \n",
    "            Covariance matrix of shape (ndim,ndim).\n",
    "    \"\"\"\n",
    "\n",
    "    cov = np.zeros((ndim,ndim))\n",
    "    diag_cov = np.ones(ndim)\n",
    "    np.fill_diagonal(cov, diag_cov)\n",
    "    \n",
    "    return cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the final function init_cov is used to randomly assign a diagonal covariance proportional to the identiy matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define parameters for emcee and harmonic machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for emcee sampling\n",
    "ndim = 10                   # number of dimensions\n",
    "nchains = 200               # total number of chains to compute\n",
    "samples_per_chain = 5000    # number of samples per chain\n",
    "nburn = 2000                # number of samples to discard as burn in\n",
    "\n",
    "# Initialize random seed\n",
    "np.random.seed(10)\n",
    "\n",
    "# Hypersphere hyperparameters\n",
    "max_r_prob = np.sqrt(ndim-1)\n",
    "domains_sphere = [max_r_prob*np.array([1E0,2E1])]\n",
    "hyper_parameters_sphere = [None]\n",
    "\n",
    "# Create covariance matrix \n",
    "cov = init_cov(ndim)\n",
    "inv_cov = np.linalg.inv(cov) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for absolute simplicity lets use the hypersphere model and fix the radius *i.e.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hm.model.HyperSphere(ndim, domains_sphere)\n",
    "model.set_R(np.sqrt(ndim)) # A good rule of thumb for diagonal covaraince Gaussians.\n",
    "model.fitted = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to run the sampler to collect samples but we wish to checkpoint periodically to protect against system crashes. One simple way to do this is to execute the following loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial random position and state\n",
    "pos = np.random.rand(ndim * nchains).reshape((nchains, ndim)) * 0.1   \n",
    "rstate = np.random.get_state()\n",
    "\n",
    "# Define how often you want to be benchmarking the evidence class\n",
    "chain_is = 10\n",
    "\n",
    "# Instantiate the evidence class\n",
    "cal_ev = hm.Evidence(nchains, model)\n",
    "\n",
    "for chain_i in range(chain_is):\n",
    "    # Run the emcee sampler from previous endpoint\n",
    "    sampler = emcee.EnsembleSampler(nchains, ndim, ln_Posterior, args=[inv_cov])\n",
    "    (pos, prob, rstate) = sampler.run_mcmc(pos, samples_per_chain/chain_is, rstate0=rstate)\n",
    "    \n",
    "    # Collect and format samples\n",
    "    samples = np.ascontiguousarray(sampler.chain[:,:,:])\n",
    "    lnprob = np.ascontiguousarray(sampler.lnprobability[:,:])\n",
    "    chains = hm.Chains(ndim)\n",
    "    chains.add_chains_3d(samples, lnprob)\n",
    "    \n",
    "    # 1) Deserialize the Evidence Class\n",
    "    if chain_i > 0:\n",
    "        cal_ev = hm.Evidence.deserialize(\".temp.gaussian_example_{}.dat\".format(ndim))\n",
    "    \n",
    "    # 2) Add these new chains to Evidence class\n",
    "    cal_ev.add_chains(chains)\n",
    "    \n",
    "    # 3) Serialize the Evidence Class \n",
    "    cal_ev.serialize(\".temp.gaussian_example_{}.dat\".format(ndim))\n",
    "    \n",
    "    # Clear memory \n",
    "    del chains, samples, lnprob, sampler, prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Bayesian evidence\n",
    "\n",
    "Finally we simply compute the Learnt Harmonic Mean estimator by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_evidence, ln_evidence_std = cal_ev.compute_ln_evidence()\n",
    "\n",
    "# Also lets compute the analytic evidence whilst we're at it.\n",
    "ln_evidence_ana = ln_analytic_evidence(ndim, cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis\n",
    "\n",
    "Lets take a look at what the error on our estimate is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Analytic value: {} || Learnt Harmonic Mean value: {}'.format(ln_evidence_ana, ln_evidence))\n",
    "\n",
    "# Compare to the learnt harmonic mean estimate \n",
    "abs_error = np.abs(ln_evidence_ana - ln_evidence)\n",
    "percent_error = 100.0 * abs_error / ln_evidence_ana\n",
    "\n",
    "# Compare the variance provided by the learnt harmonic mean estimate\n",
    "sigma_error = np.log(np.exp(ln_evidence) + np.exp(ln_evidence_std)) - ln_evidence\n",
    "\n",
    "sigma_count = abs_error / sigma_error\n",
    "\n",
    "print('Number of sigma away from true value: {}'.format(sigma_count))\n",
    "\n",
    "# N.B: sigma_count represents the number of standard deviations between the estimated \n",
    "#     and analytic results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
